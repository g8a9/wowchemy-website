---
# Documentation: https://wowchemy.com/docs/managing-content/

title: "From Recurrent Models to the Advent of Attention: A Recap"
event:
event_url:
location:
address:
  street:
  city:
  region:
  postcode:
  country:
summary:
abstract:

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: 2020-12-18T17:00:13+02:00
# date_end: 2020-12-18T17:28:13+02:00
all_day: false

# Schedule page publish date (NOT event date).
publishDate: 2021-08-28T17:28:13+02:00

authors:
- Giuseppe Attanasio
tags: []

# Is this a featured event? (true/false)
featured: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

# Custom links (optional).
#   Uncomment and edit lines below to show custom links.
# links:
# - name: Follow
#   url: https://twitter.com
#   icon_pack: fab
#   icon: twitter

# Optional filename of your slides within your event's folder or a URL.
url_slides: https://drive.google.com/file/d/1-6c9KocteBRIrz3A4OF8_7MueeooKijN/view?usp=sharing

url_code:
url_pdf:
url_video: https://youtu.be/v4dqhP6HVns

# Markdown Slides (optional).
#   Associate this event with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: ""

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
---

In this talk, I present a chronological overview of modeling solutions for time series and natural language. I go through seminal papers on the applications of **recurrent models** to tasks like language modeling, neural machine translation, image captioning, or multi-model text generation.
Next, I describe the advantages and shortcomings of RNNs, motivating why the latter brought to the advent of the **Attention** mechanism. I conclude with a brief introduction to the **Transformer** architecture.

The talk is part of Research Bites, the series of research-oriented seminaries we organized for students of the course Data Science Lab: process and methods. 
The other talks in the series were given by Andrea Pasini (on Image Segmentation), Francesco Ventura (on Explainable AI), Eliana Pastor (on ML pipelines in production), Flavio Giobergia (on word embeddings), and Moreno La Quatra (on GANs).